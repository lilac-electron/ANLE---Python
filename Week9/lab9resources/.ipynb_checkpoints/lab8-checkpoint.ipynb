{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rural-gasoline",
   "metadata": {},
   "source": [
    "# Week 8: Using Transformer Models\n",
    "\n",
    "\n",
    "## Getting started\n",
    "If working on your own machine, make sure the huggingface transformers package is installed\n",
    "\n",
    "`conda install -c huggingface transformers`\n",
    "\n",
    "or\n",
    "\n",
    "`pip install transformers`\n",
    "\n",
    "Of course, if working on Google Colab, you won't need to do this.  Whatever environment you are using check whether the following code runs.  It should output a negative label with a high score!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-fireplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "print(pipeline('sentiment-analysis')('I hate you'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-joshua",
   "metadata": {},
   "source": [
    "The following is adapted from the huggingface quickstart to transformers tutorial https://huggingface.co/transformers/quickstart.html\n",
    "We will be looking at the BERT introduction (but feel free to have a look at GPT2 etc as well!)\n",
    "\n",
    "First of all we need some key imports.  We are going to be using the pre-trained bert-base-uncased model so this cell instantiates a tokenizer for this model.  Logging is also switched on so we can see more of what's going on. The first time you run it, the model will be downloaded and cached.  The cached version will be used on subsequent runs, if it is available (not on Google CoLab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "certified-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening under the hood, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-lesbian",
   "metadata": {},
   "source": [
    "Now we are going to tokenize some text.  This will demonstrate the 'wordpiece' vocabulary used by BERT as well as the fact that we need to introduce special `[CLS]` and `[SEP]` tokens in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "buried-sponsorship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'who', 'was', 'elected', 'as', 'british', 'prime', 'minister', 'in', '1951', '?', '[SEP]', 'sir', 'winston', 'leonard', 'spencer', 'churchill', 'was', 'a', 'british', 'politician', ',', 'statesman', ',', 'army', 'officer', 'and', 'writer', ',', 'who', 'was', 'prime', 'minister', 'of', 'the', 'united', 'kingdom', 'from', '1940', 'to', '1945', 'and', 'again', 'from', '1951', 'to', '1955', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize input\n",
    "text = \"[CLS] Who was elected as British prime minister in 1951? [SEP] Sir Winston Leonard Spencer Churchill was a British politician, statesman, army officer and writer, who was Prime Minister of the United Kingdom from 1940 to 1945 and again from 1951 to 1955. [SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "binding-diana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'what', 'are', 'ign', '##eous', 'rocks', '?', '[SEP]', 'ign', '##eous', 'rocks', 'form', 'when', 'hot', ',', 'molten', 'rock', 'crystal', '##li', '##zes', 'and', 'solid', '##ifies', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize input\n",
    "text = \"[CLS] What are igneous rocks? [SEP] Igneous rocks form when hot , molten rock crystallizes and solidifies. [SEP] \"\n",
    "tokenized_text= tokenizer.tokenize(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-dealing",
   "metadata": {},
   "source": [
    "Note that the tokenizer is not breaking down all words according to their morphology -- only rare words.  Reasonably frequent words such as `elected` are left as whole words.  Rarer words such as `solidifies` are broken down.\n",
    "\n",
    "Now we are going to mask out one of the words in the text.  For the purposes of this demonstration, I have chosen token 11 but you could try different tokens.  Remember that during training the tokens to mask are chosen randomly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-valuation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "masked_index = 11\n",
    "tokenized_text[masked_index] = '[MASK]'\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-forwarding",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tokenized_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-completion",
   "metadata": {},
   "source": [
    "We are now going to try to use the masked language model to predict this word.\n",
    "\n",
    "First we need to convert the input into a list of word index ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-playing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert token to vocabulary indices\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "print(indexed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-papua",
   "metadata": {},
   "source": [
    "We need segment ids to define whether a token is in the first or second sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-croatia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_segment_ids(list_of_tokens):\n",
    "    #this function assumes that up to and including the first '[SEP]' is the first segment, anything afterwards is the second segment\n",
    "    current_id=0\n",
    "    segment_ids=[]\n",
    "    for token in list_of_tokens:\n",
    "        segment_ids.append(current_id)\n",
    "        if token == '[SEP]':\n",
    "            current_id=1\n",
    "    return segment_ids\n",
    "\n",
    "segment_ids=make_segment_ids(tokenized_text)\n",
    "print(segment_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-bundle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert inputs to PyTorch tensors\n",
    "#this just wraps things up in multi-dimensional tensors rather than as flat lists.\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segment_ids])\n",
    "print(tokens_tensor)\n",
    "print(segments_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-motivation",
   "metadata": {},
   "source": [
    "Now we need to encode the input using the bert-base-uncased model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "present-apparatus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Set the model in evaluation mode to deactivate the DropOut modules\n",
    "# This is IMPORTANT to have reproducible results during evaluation!\n",
    "model.eval()\n",
    "\n",
    "# If you have a GPU, put everything on cuda - otherwise comment this out to run on CPU\n",
    "#tokens_tensor = tokens_tensor.to('cuda')\n",
    "#segments_tensors = segments_tensors.to('cuda')\n",
    "#model.to('cuda')\n",
    "\n",
    "# Predict hidden states features for each layer\n",
    "with torch.no_grad():\n",
    "    # See the models docstrings for the detail of the inputs\n",
    "    outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "    # Transformers models always output tuples.\n",
    "    # See the models docstrings for the detail of all the outputs\n",
    "    # In our case, the first element of outputs is the output of the last layer of the Bert model (all tokens)\n",
    "    # the second element of outputs, outputs[1] is actually just a \"pooled_output\" representation of the CLS token (rather than all tokens) - however this involves an extra layer which is why it is not the same as the first element in outputs[0]!\n",
    "    encoded_layers = outputs[0]\n",
    "# We have encoded our input sequence in a FloatTensor of shape (batch size, sequence length, model hidden dimension)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powerful-today",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have encoded our input sequence in a FloatTensor of shape (batch size, sequence length, model hidden dimension)\n",
    "print(encoded_layers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-orlando",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-shoot",
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputs[1] is a representation of the CLS token of shape (batch size, model hidden dimension)\n",
    "outputs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-demographic",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-intelligence",
   "metadata": {},
   "source": [
    "We can also predict the masked token as follows.  We make the predictions as before (using the last layer of the BERT model) but then we find the token id which maximises the prediction for the masked token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "postal-genealogy",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "# If you have a GPU, put everything on cuda\n",
    "#tokens_tensor = tokens_tensor.to('cuda')\n",
    "#segments_tensors = segments_tensors.to('cuda')\n",
    "#model.to('cuda')\n",
    "\n",
    "# Predict all tokens\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "    predictions = outputs[0]\n",
    "\n",
    "        \n",
    "# find the token id which maximises the prediction for the masked token and then convert this back to a word\n",
    "predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "print(predicted_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peaceful-handbook",
   "metadata": {},
   "source": [
    "### Exercise 0\n",
    "Mask each token in turn and see what BERT predicts.   How accurate are its predictions?  As an extension, you could look at masking multiple words in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-johnston",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "threaded-muslim",
   "metadata": {},
   "source": [
    "## Representing Sentential Meaning\n",
    "We are going to be looking at different strategies for representing sentential meaning\n",
    "* CLS token representation\n",
    "* centroid/sum of output embeddings\n",
    "\n",
    "The file `examples.txt` contains some example sentences.\n",
    "\n",
    "### Exercise 1\n",
    "Read in the sentences and store them as a list of sentences.  Add `[CLS]` and `[SEP]` tokens to the beginning and end of each and then pass them through the bert-base-uncased tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-large",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-employer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "basic-direction",
   "metadata": {},
   "source": [
    "When encoding sentences, it is actually more typical to pool the hidden states for each layer (at depth n) rather than the output layer.  We can access the hidden states of the model using `output_hidden_states=True` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-effort",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Predict hidden states features for each layer\n",
    "with torch.no_grad():\n",
    "    # See the models docstrings for the detail of the inputs\n",
    "    outputs = model(tokens_tensor, token_type_ids=segments_tensors,output_hidden_states=True)\n",
    "   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-juvenile",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.to_tuple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-protocol",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(outputs))\n",
    "for i in range(len(outputs)):\n",
    "    try:\n",
    "        print(outputs[i].shape)\n",
    "    except:\n",
    "        print(len(outputs[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-wright",
   "metadata": {},
   "source": [
    "Here:\n",
    "* outputs[0] contains the output representation of each token\n",
    "* outputs[1] is representation of the first token (after being put through an additional layer)\n",
    "* outputs[2] is a a tuple.  Each element is the hidden layer at depth n.  If we want the last layer then we need outputs[2][-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-kazakhstan",
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputs[2][-1] is the last hidden layer also output as outputs[0]\n",
    "outputs[2][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-category",
   "metadata": {},
   "outputs": [],
   "source": [
    "#so if you want the penultimate hidden layer you need outputs[2][-2]\n",
    "outputs[2][-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exotic-grant",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "exposed-humor",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "* Encode each sentence using the output representation for its CLS token - note that you do not need to mask the CLS token.  We are just interested in the output layer embedding for this token.  You can use outputs[0][0] or outputs[1] as a representation of the CLS token - but you will get different results as outputs[1] as gone through an additional layer (trained for next sentence prediction during fine-tuning and classification IF the model has been fine-tuned).\n",
    "* Use cosine similarity to determine all pairs similarities for the sentences.\n",
    "* Identify the 10 most similar pairs of sentences using this sentence encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-transsexual",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this is a handy way of finding the cosine similarity between two tensors\n",
    "# see https://pytorch.org/docs/stable/generated/torch.nn.CosineSimilarity.html\n",
    "cos = torch.nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "\n",
    "#you use this as:\n",
    "print(encoded_layers[0,0],encoded_layers[0,1])\n",
    "output=cos(encoded_layers[0,0],encoded_layers[0,1])\n",
    "print(output.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-directive",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-kingdom",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worse-calcium",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "durable-joint",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "a) Repeat exercise 2 but use the centroid of all of the output embeddings as the representation of a sentence.\n",
    "\n",
    "b) Experiment with using different pooling layers from the hidden state embeddings.  Typically, using the penultimate layer (-2) is felt to be optimal as it is far enough away from the original uncontextualised word embeddings but also not too close to the output predictions.  See here for a discussion: https://github.com/hanxiao/bert-as-service#q-what-are-the-available-pooling-strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-debate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interim-moses",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims=run(sentences,poolinglayer=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-tribe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sims=run(sentences,method=\"cls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-sauce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-contributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sims[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-facility",
   "metadata": {},
   "outputs": [],
   "source": [
    "interested=[2,1,21,7,8,22,3,25]\n",
    "for i in interested:\n",
    "    print(sentences[i],sims[0][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-dodge",
   "metadata": {},
   "source": [
    "### Extension 1\n",
    "The MRPC.zip file contains a training, dev and test split for the Microsoft Research paraphrase corpus.  In this corpus the quality '1' indicates that the 2 sentences are considered to be paraphrases and '0' indicates that they are not.\n",
    "\n",
    "Can you build a classifier on top of the BERT pre-trained model, trained on the training split of MRPC, which predicts whether 2 sentences are paraphrases or not?\n",
    "\n",
    "Note this does not require you to fine-tune the BERT model.  You can use outputs from BERT as input to your separate classifier.  I would suggest a single neural layer which uses the representation from exercise 2 or 3 as input, built using scikit-learn or torch.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-portuguese",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
