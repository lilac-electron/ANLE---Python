{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Lab 6: Neural Language Models\n",
    "\n",
    "This week we are going to be looking at using the pytorch library to build a simple feedforward neural language model.  This notebook is adapted from one of the pytorch tutorials and includes code by Robert Guthrie as well as my own.\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html#sphx-glr-beginner-nlp-word-embeddings-tutorial-py\n",
    "\n",
    "\n",
    "### Word Embeddings in Pytorch\n",
    "\n",
    "Before we get to a worked example and some exercises, a few quick notes\n",
    "about how to use embeddings in Pytorch.  First, we need to define an index for each word\n",
    "when using embeddings. These will be keys into a lookup table. That is,\n",
    "embeddings are stored as a $|V| \\times D$ matrix, where $D$\n",
    "is the dimensionality of the embeddings, such that the word assigned\n",
    "index $i$ has its embedding stored in the $i$'th row of the\n",
    "matrix. In all of my code, the mapping from words to indices is a\n",
    "dictionary named word\\_to\\_ix.\n",
    "\n",
    "The module that allows you to use embeddings is torch.nn.Embedding,\n",
    "which takes two arguments: the vocabulary size, and the dimensionality\n",
    "of the embeddings.\n",
    "\n",
    "To index into this table, you must use torch.LongTensor (since the\n",
    "indices are integers, not floats).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9aa85641f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard pytorch imports\n",
    "# Author: Robert Guthrie\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
    "embeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings\n",
    "lookup_tensor = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long)\n",
    "hello_embed = embeds(lookup_tensor)\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1661, -1.5228,  0.3817, -1.0276, -0.5631]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "current_tensor = torch.tensor([word_to_ix[\"world\"]], dtype =torch.long)\n",
    "print(embeds(current_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Example: N-Gram Language Modeling\n",
    "\n",
    "\n",
    "Recall that in an n-gram language model, given a sequence of words\n",
    "$w$, we want to compute\n",
    "\n",
    "\\begin{align}P(w_i | w_{i-1}, w_{i-2}, \\dots, w_{i-n+1} )\\end{align}\n",
    "\n",
    "where $w_i$ is the ith word of the sequence.\n",
    "\n",
    "In this example, we will compute the loss function on some training\n",
    "examples and update the parameters with backpropagation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([\"feel'st\", 'it'], 'cold'), (['it', 'cold'], '.'), (['cold', '.'], '__END')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize as tokenize\n",
    "\n",
    "CONTEXT_SIZE = 2  #this is the amount of preceding context to consider\n",
    "EMBEDDING_DIM = 10  #this is the dimension of the embeddings\n",
    "# We will use Shakespeare Sonnet 2\n",
    "test_sentence = [\"__END\",\"__START\"]+tokenize(\"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\")+[\"__END\"]\n",
    "\n",
    "# build a list of tuples.  Each tuple is ([ word_i-2, word_i-1 ], target word)\n",
    "trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n",
    "            for i in range(len(test_sentence) - 2)]\n",
    "# print the last 3, just so you can see what they look like\n",
    "print(trigrams[-3:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to find the set of words making up the vocabulary and create the word_to_ix index.  We'll also make a reverse index ix_to_word at the same time so that we can look up a word associated with an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'forty': 0,\n",
       " 'thriftless': 1,\n",
       " 'And': 2,\n",
       " 'thine': 3,\n",
       " 'sum': 4,\n",
       " 'Were': 5,\n",
       " 'the': 6,\n",
       " 'fair': 7,\n",
       " 'new': 8,\n",
       " 'where': 9,\n",
       " 'totter': 10,\n",
       " 'eyes': 11,\n",
       " ';': 12,\n",
       " 'Thy': 13,\n",
       " 'When': 14,\n",
       " 'his': 15,\n",
       " 'Will': 16,\n",
       " 'all': 17,\n",
       " 'when': 18,\n",
       " 'shall': 19,\n",
       " 'thou': 20,\n",
       " 'treasure': 21,\n",
       " '__START': 22,\n",
       " 'field': 23,\n",
       " 'art': 24,\n",
       " 'asked': 25,\n",
       " \"'d\": 26,\n",
       " 'child': 27,\n",
       " 'youth': 28,\n",
       " 'weed': 29,\n",
       " 'succession': 30,\n",
       " 'sunken': 31,\n",
       " \"'s\": 32,\n",
       " 'Proving': 33,\n",
       " 'being': 34,\n",
       " 'worth': 35,\n",
       " '.': 36,\n",
       " 'much': 37,\n",
       " 'gazed': 38,\n",
       " 'shame': 39,\n",
       " 'livery': 40,\n",
       " 'If': 41,\n",
       " \"feel'st\": 42,\n",
       " 'Where': 43,\n",
       " 'days': 44,\n",
       " 'deserv': 45,\n",
       " 'now': 46,\n",
       " 'an': 47,\n",
       " 'To': 48,\n",
       " 'count': 49,\n",
       " 'dig': 50,\n",
       " 'lusty': 51,\n",
       " 'make': 52,\n",
       " 'trenches': 53,\n",
       " 'all-eating': 54,\n",
       " 'mine': 55,\n",
       " 'be': 56,\n",
       " 'more': 57,\n",
       " 'in': 58,\n",
       " 'This': 59,\n",
       " 'old': 60,\n",
       " 'deep': 61,\n",
       " 'Shall': 62,\n",
       " 'own': 63,\n",
       " 'so': 64,\n",
       " 'thy': 65,\n",
       " 'to': 66,\n",
       " '!': 67,\n",
       " 'a': 68,\n",
       " 'excuse': 69,\n",
       " 'How': 70,\n",
       " 'and': 71,\n",
       " 'warm': 72,\n",
       " 'say': 73,\n",
       " 'blood': 74,\n",
       " 'proud': 75,\n",
       " 'were': 76,\n",
       " \"'\": 77,\n",
       " 'winters': 78,\n",
       " 'praise': 79,\n",
       " 'lies': 80,\n",
       " 'within': 81,\n",
       " 'couldst': 82,\n",
       " 'brow': 83,\n",
       " 'it': 84,\n",
       " '__END': 85,\n",
       " 'besiege': 86,\n",
       " 'Then': 87,\n",
       " 'held': 88,\n",
       " 'answer': 89,\n",
       " 'of': 90,\n",
       " \"'This\": 91,\n",
       " 'by': 92,\n",
       " 'cold': 93,\n",
       " 'small': 94,\n",
       " 'beauty': 95,\n",
       " 'see': 96,\n",
       " 'made': 97,\n",
       " 'use': 98,\n",
       " ':': 99,\n",
       " ',': 100,\n",
       " 'my': 101,\n",
       " 'on': 102}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find the vocabulary and create the index\n",
    "vocab = set(test_sentence)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "ix_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "word_to_ix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our basic NGramLanguageModeler class.  It inherits from the nn.Module class\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
    "\n",
    "Essentially, the ``__init__`` method is used to define the neural network.  We have a set of embeddings (vocab_size by embedding_dim) and then 2 linear layers.  The first (or hidden) layer has 128 neurons each with context_size * embedding_dim inputs.  The size of the second layer is equal to the vocab_size, where each neuron has 128 inputs (one from each neuron in the preceding layer).  The value at each of the neurons in this output layer will tell us the probability of each word in the vocabulary as the next word in the sequence.\n",
    "\n",
    "The ``forward`` method is used to run the network in forward mode i.e., give it some inputs and get some outputs.  Activation functions are added to each layer - the hidden layer has a relu function applied to each neuron and the output layer outputs go through a softmax in order to create a probability distribution.\n",
    "\n",
    "The ``train`` method iterates over the corpus for a certain number of epochs.  The embeddings for the current context are selected and passed to the model's ``forward`` method.  The log probability of the current target word according to the output is used to compute the loss (i.e., how likely is the target word given the current parameters) and this is then back-propagated through the network via stochastic gradient descent.  It also prints the losses on each epoch - so you can see whether this is decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[650.6992130279541, 646.4588539600372, 642.2797882556915, 638.162456035614, 634.106636762619, 630.111043214798, 626.1730239391327, 622.2942018508911, 618.4740650653839, 614.7137267589569]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "\n",
    "    def train(self,inputngrams,loss_function=nn.NLLLoss(),lr=0.001,epochs=10):\n",
    "        optimizer=optim.SGD(self.parameters(),lr=lr)\n",
    "        \n",
    "        losses=[]\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for context, target in inputngrams:\n",
    "\n",
    "                # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "                # into integer indices and wrap them in tensors)\n",
    "                context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "\n",
    "                # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "                # new instance, you need to zero out the gradients from the old\n",
    "                # instance\n",
    "                self.zero_grad()\n",
    "\n",
    "                # Step 3. Run the forward pass, getting log probabilities over next\n",
    "                # words\n",
    "                log_probs = self.forward(context_idxs)\n",
    "\n",
    "                # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "                # word wrapped in a tensor)\n",
    "                loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
    "\n",
    "                # Step 5. Do the backward pass and update the gradient\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "                # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "                total_loss += loss.item()\n",
    "            losses.append(total_loss)\n",
    "        print(losses)\n",
    "\n",
    "\n",
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "model.train(trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to some generation with the model.  I've added some extra methods to the class which reflect the methods we had in our ngram language model in week 4.  See if you can work out what each step is doing in each of:\n",
    "* `get_logprob()`\n",
    "* `nextlikely()`\n",
    "* `generate()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math,random\n",
    "\n",
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "    def get_logprob(self,context,target):\n",
    "        #return the logprob of the target word given the context\n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "        log_probs = self.forward(context_idxs)\n",
    "        target_idx=torch.tensor(word_to_ix[target],dtype=torch.long)\n",
    "        return log_probs.index_select(1,target_idx).item()\n",
    "        \n",
    "        \n",
    "    def nextlikely(self,context):\n",
    "        #sample the distribution of target words given the context\n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "        log_probs = self.forward(context_idxs)\n",
    "        probs=[math.exp(x) for x in log_probs.flatten().tolist()]\n",
    "        t=random.choices(list(range(len(probs))),weights=probs,k=1)\n",
    "        return ix_to_word[t[0]]\n",
    "    \n",
    "    def generate(self,limit=20):\n",
    "        #generate a sequence of tokens according to the model\n",
    "        tokens=[\"__END\",\"__START\"]\n",
    "        while tokens[-1]!=\"__END\" and len(tokens)<limit:\n",
    "            current=self.nextlikely(tokens[-2:])\n",
    "            tokens.append(current)\n",
    "        return \" \".join(tokens[2:-1])\n",
    "    \n",
    "    def train(self,inputngrams,loss_function=nn.NLLLoss(),lr=0.001,epochs=10):\n",
    "        optimizer=optim.SGD(self.parameters(),lr=lr)\n",
    "        \n",
    "        losses=[]\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for context, target in inputngrams:\n",
    "\n",
    "                # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "                # into integer indices and wrap them in tensors)\n",
    "                context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "\n",
    "                # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "                # new instance, you need to zero out the gradients from the old\n",
    "                # instance\n",
    "                self.zero_grad()\n",
    "\n",
    "                # Step 3. Run the forward pass, getting log probabilities over next\n",
    "                # words\n",
    "                log_probs = self.forward(context_idxs)\n",
    "\n",
    "                # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "                # word wrapped in a tensor)\n",
    "                loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
    "\n",
    "                # Step 5. Do the backward pass and update the gradient\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "                # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "                total_loss += loss.item()\n",
    "            losses.append(total_loss)\n",
    "        print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[656.9471893310547, 652.7292160987854, 648.5683555603027, 644.4635548591614, 640.4146265983582, 636.4171857833862, 632.468772649765, 628.5719289779663, 624.7245268821716, 620.9261801242828]\n"
     ]
    }
   ],
   "source": [
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "model.train(trigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.344061851501465"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_logprob([\"his\",\"field\"],\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word=model.nextlikely([\"his\",\"field\"])\n",
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\", small answer blood warm 'This own blood count forty Thy sunken Then Shall Shall totter own\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "* Train your neural language model on the training split of the corpus for the Microsoft Research Sentence Completion Challenge (see lab 2).\n",
    "* Generate some likely sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this will take a long time to run even if you only give it one file to process.  Reducing the size of the vocabulary (in exercise 2) will improve the run time and the ability of the model to generalise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 522 files in the training directory: /Users/juliewe/Dropbox/teaching/AdvancedNLP/2024/week4/lab4/lab4resources/sentence-completion/Holmes_Training_Data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "TRAINING_DIR=\"/Users/juliewe/Dropbox/teaching/AdvancedNLP/2024/week4/lab4/lab4resources/sentence-completion/Holmes_Training_Data\"  #this needs to be the parent directory for the training corpus\n",
    "\n",
    "def get_training_testing(training_dir=TRAINING_DIR,split=0.5):\n",
    "\n",
    "    filenames=os.listdir(training_dir)\n",
    "    n=len(filenames)\n",
    "    print(\"There are {} files in the training directory: {}\".format(n,training_dir))\n",
    "    random.seed(53)  #if you want the same random split every time\n",
    "    random.shuffle(filenames)\n",
    "    index=int(n*split)\n",
    "    return(filenames[:index],filenames[index:])\n",
    "\n",
    "trainingfiles,heldoutfiles=get_training_testing()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainingfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embedding_dim=embedding_dim\n",
    "        self.context_size=context_size\n",
    "        self.hidden_size=128\n",
    "        \n",
    "    def initialise(self):\n",
    "        self.embeddings = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.linear1 = nn.Linear(self.context_size * self.embedding_dim, self.hidden_size)\n",
    "        self.linear2 = nn.Linear(self.hidden_size, self.vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "    def get_logprob(self,context,target):\n",
    "        #return the logprob of the target word given the context\n",
    "        context_idxs = torch.tensor([self.word_to_ix[w] for w in context], dtype=torch.long)\n",
    "        log_probs = self.forward(context_idxs)\n",
    "        target_idx=torch.tensor(self.word_to_ix[target],dtype=torch.long)\n",
    "        return log_probs.index_select(1,target_idx).item()\n",
    "        \n",
    "        \n",
    "    def nextlikely(self,context):\n",
    "        #sample the distribution of target words given the context\n",
    "        context_idxs = torch.tensor([self.word_to_ix[w] for w in context], dtype=torch.long)\n",
    "        log_probs = self.forward(context_idxs)\n",
    "        probs=[math.exp(x) for x in log_probs.flatten().tolist()]\n",
    "        t=random.choices(list(range(len(probs))),weights=probs,k=1)\n",
    "        return self.ix_to_word[t[0]]\n",
    "    \n",
    "    def generate(self,limit=20):\n",
    "        #generate a sequence of tokens according to the model\n",
    "        tokens=[\"__END\",\"__START\"]\n",
    "        while tokens[-1]!=\"__END\" and len(tokens)<limit:\n",
    "            current=self.nextlikely(tokens[-2:])\n",
    "            tokens.append(current)\n",
    "        return \" \".join(tokens[2:-1])\n",
    "    \n",
    "    def train(self,inputngrams,loss_function=nn.NLLLoss(),lr=0.001,epochs=3):\n",
    "        optimizer=optim.SGD(self.parameters(),lr=lr)\n",
    "        \n",
    "        losses=[]\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for context, target in inputngrams:\n",
    "\n",
    "                # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "                # into integer indices and wrap them in tensors)\n",
    "                context_idxs = torch.tensor([self.word_to_ix[w] for w in context], dtype=torch.long)\n",
    "\n",
    "                # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "                # new instance, you need to zero out the gradients from the old\n",
    "                # instance\n",
    "                self.zero_grad()\n",
    "\n",
    "                # Step 3. Run the forward pass, getting log probabilities over next\n",
    "                # words\n",
    "                log_probs = self.forward(context_idxs)\n",
    "\n",
    "                # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "                # word wrapped in a tensor)\n",
    "                loss = loss_function(log_probs, torch.tensor([self.word_to_ix[target]], dtype=torch.long))\n",
    "\n",
    "                # Step 5. Do the backward pass and update the gradient\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "                # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "                total_loss += loss.item()\n",
    "            losses.append(total_loss)\n",
    "            print(\"Completed epoch {} with loss {}\".format(epoch,total_loss))\n",
    "        return losses\n",
    "        \n",
    "    \n",
    "    def train_from_corpus(self,training_dir=TRAINING_DIR,files=[]):\n",
    "        alltokens=[\"__END\"]\n",
    "        #reading corpus and tokenize\n",
    "        for afile in files:\n",
    "            print(\"Reading {}\".format(afile))\n",
    "            try:\n",
    "                with open(os.path.join(training_dir,afile)) as instream:\n",
    "                    for line in instream:\n",
    "                        line=line.rstrip()\n",
    "                        if len(line)>0:\n",
    "                            tokens=[\"__START\"]+tokenize(line)+[\"__END\"]\n",
    "                            alltokens+=tokens\n",
    "            except UnicodeDecodeError:\n",
    "                print(\"UnicodeDecodeError reading {}: ignoring file\".format(afile))\n",
    "        \n",
    "        \n",
    "        #get the vocab and build the indexes\n",
    "        self.vocab = set(alltokens)\n",
    "        self.word_to_ix = {word: i for i, word in enumerate(self.vocab)}\n",
    "        self.ix_to_word = {i: word for i, word in enumerate(self.vocab)}\n",
    "        \n",
    "        #MUST SET THE VOCAB SIZE and INITIALISE THE NN\n",
    "        self.vocab_size=len(self.vocab) \n",
    "        print(\"Vocabulary size is {}\".format(self.vocab_size))\n",
    "        self.initialise()\n",
    "        \n",
    "        #convert to trigrams\n",
    "        trigrams = [([alltokens[i], alltokens[i + 1]], alltokens[i + 2])\n",
    "            for i in range(len(alltokens) - 2)]\n",
    "        \n",
    "        print(\"Starting training\")\n",
    "        #train using the trigrams\n",
    "        self.train(trigrams)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 19TOM10.TXT\n",
      "Vocabulary size is 5280\n",
      "Starting training\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m MAX_FILES\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m NGramLanguageModeler(EMBEDDING_DIM, CONTEXT_SIZE)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_from_corpus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainingfiles\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mMAX_FILES\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 113\u001b[0m, in \u001b[0;36mNGramLanguageModeler.train_from_corpus\u001b[0;34m(self, training_dir, files)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m#train using the trigrams\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrigrams\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 71\u001b[0m, in \u001b[0;36mNGramLanguageModeler.train\u001b[0;34m(self, inputngrams, loss_function, lr, epochs)\u001b[0m\n\u001b[1;32m     68\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(log_probs, torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_to_ix[target]], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong))\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Step 5. Do the backward pass and update the gradient\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Get the Python number from a 1-element Tensor by calling tensor.item()\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MAX_FILES=1\n",
    "model = NGramLanguageModeler(EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "model.train_from_corpus(files=trainingfiles[:MAX_FILES])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "* Modify your model so that all words in the vocabulary with frequency less than a threshold (e.g, 20) are replaced by the \"\\_\\_UNK\" token\n",
    "* Generate some likely sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embedding_dim=embedding_dim\n",
    "        self.context_size=context_size\n",
    "        self.hidden_size=128\n",
    "        self.threshold=20\n",
    "        \n",
    "    def initialise(self):\n",
    "        self.embeddings = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.linear1 = nn.Linear(self.context_size * self.embedding_dim, self.hidden_size)\n",
    "        self.linear2 = nn.Linear(self.hidden_size, self.vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "    def get_logprob(self,context,target):\n",
    "        #return the logprob of the target word given the context\n",
    "        context_idxs = torch.tensor([self.word_to_ix[w] for w in context], dtype=torch.long)\n",
    "        log_probs = self.forward(context_idxs)\n",
    "        target_idx=torch.tensor(self.word_to_ix[target],dtype=torch.long)\n",
    "        return log_probs.index_select(1,target_idx).item()\n",
    "        \n",
    "        \n",
    "    def nextlikely(self,context):\n",
    "        #sample the distribution of target words given the context\n",
    "        context_idxs = torch.tensor([self.word_to_ix[w] for w in context], dtype=torch.long)\n",
    "        log_probs = self.forward(context_idxs)\n",
    "        probs=[math.exp(x) for x in log_probs.flatten().tolist()]\n",
    "        t=random.choices(list(range(len(probs))),weights=probs,k=1)\n",
    "        return self.ix_to_word[t[0]]\n",
    "    \n",
    "    def generate(self,limit=20):\n",
    "        #generate a sequence of tokens according to the model\n",
    "        tokens=[\"__END\",\"__START\"]\n",
    "        while tokens[-1]!=\"__END\" and len(tokens)<limit:\n",
    "            current=self.nextlikely(tokens[-2:])\n",
    "            tokens.append(current)\n",
    "        return \" \".join(tokens[2:-1])\n",
    "    \n",
    "    def train(self,inputngrams,loss_function=nn.NLLLoss(),lr=0.001,epochs=3):\n",
    "        optimizer=optim.SGD(self.parameters(),lr=lr)\n",
    "        \n",
    "        losses=[]\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for context, target in inputngrams:\n",
    "\n",
    "                # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "                # into integer indices and wrap them in tensors)\n",
    "                context_idxs = torch.tensor([self.word_to_ix[w] for w in context], dtype=torch.long)\n",
    "\n",
    "                # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "                # new instance, you need to zero out the gradients from the old\n",
    "                # instance\n",
    "                self.zero_grad()\n",
    "\n",
    "                # Step 3. Run the forward pass, getting log probabilities over next\n",
    "                # words\n",
    "                log_probs = self.forward(context_idxs)\n",
    "\n",
    "                # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "                # word wrapped in a tensor)\n",
    "                loss = loss_function(log_probs, torch.tensor([self.word_to_ix[target]], dtype=torch.long))\n",
    "\n",
    "                # Step 5. Do the backward pass and update the gradient\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "                # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "                total_loss += loss.item()\n",
    "            losses.append(total_loss)\n",
    "            print(\"Completed epoch {} with loss {}\".format(epoch,total_loss))\n",
    "        return losses\n",
    "        \n",
    "    \n",
    "    def train_from_corpus(self,training_dir=TRAINING_DIR,files=[]):\n",
    "        alltokens=[\"__END\"]\n",
    "        #reading corpus and tokenize\n",
    "        for afile in files:\n",
    "            print(\"Reading {}\".format(afile))\n",
    "            try:\n",
    "                with open(os.path.join(training_dir,afile)) as instream:\n",
    "                    for line in instream:\n",
    "                        line=line.rstrip()\n",
    "                        if len(line)>0:\n",
    "                            tokens=[\"__START\"]+tokenize(line)+[\"__END\"]\n",
    "                            alltokens+=tokens\n",
    "            except UnicodeDecodeError:\n",
    "                print(\"UnicodeDecodeError reading {}: ignoring file\".format(afile))\n",
    "        \n",
    "        \n",
    "        #get the vocab and build the indexes\n",
    "        self.vocab={}\n",
    "        for token in alltokens:\n",
    "            self.vocab[token]=self.vocab.get(token,0)+1\n",
    "            \n",
    "        #delete unknown words from vocab\n",
    "        unknowns=0\n",
    "        for key,value in list(self.vocab.items()):\n",
    "            if value < self.threshold:\n",
    "                unknowns+=value\n",
    "                self.vocab.pop(key,None)\n",
    "        self.vocab[\"__UNK\"]=unknowns\n",
    "        \n",
    "        self.word_to_ix = {word: i for i, word in enumerate(list(self.vocab.keys()))}\n",
    "        self.ix_to_word = {i: word for i, word in enumerate(list(self.vocab.keys()))}\n",
    "        \n",
    "        #MUST SET THE VOCAB SIZE and INITIALISE THE NN\n",
    "        self.vocab_size=len(self.vocab) \n",
    "        print(\"Vocabulary size is {}\".format(self.vocab_size))\n",
    "        self.initialise()\n",
    "        \n",
    "        #replace unknown words\n",
    "        \n",
    "        filteredtokens=[]\n",
    "        for token in alltokens:\n",
    "            if token in self.vocab.keys():\n",
    "                filteredtokens.append(token)\n",
    "            else:\n",
    "                filteredtokens.append(\"__UNK\")\n",
    "        #convert to trigrams\n",
    "        trigrams = [([filteredtokens[i], filteredtokens[i + 1]], filteredtokens[i + 2])\n",
    "            for i in range(len(filteredtokens) - 2)]\n",
    "        \n",
    "        print(\"Starting training\")\n",
    "        #train using the trigrams\n",
    "        self.train(trigrams)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 19TOM10.TXT\n",
      "Vocabulary size is 337\n",
      "Starting training\n",
      "Completed epoch 0 with loss 259095.37446790934\n",
      "Completed epoch 1 with loss 238583.31883164006\n",
      "Completed epoch 2 with loss 231471.25745355838\n"
     ]
    }
   ],
   "source": [
    "MAX_FILES=1\n",
    "model = NGramLanguageModeler(EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "model.train_from_corpus(files=trainingfiles[:MAX_FILES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'__UNK for the __UNK of not . He to the and is'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "* Calculate the perplexity of the test corpus according to your NLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "* Try some different embedding sizes\n",
    "* Plot a graph of perplexity against embedding size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "* Extend your model so that you can consider different amounts of context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
