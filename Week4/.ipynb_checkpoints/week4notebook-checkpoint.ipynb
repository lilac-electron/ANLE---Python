{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modelling Lab (week 4)\n",
    "This notebook provides the \"starter\" code in the week 4 lab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\n",
    "We need to get the names of files in the training directory and split them into training and testing 50:50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 522 files in the training directory: lab2resources/sentence-completion/Holmes_Training_Data\n"
     ]
    }
   ],
   "source": [
    "import os,random,math\n",
    "TRAINING_DIR=\"lab2resources/sentence-completion/Holmes_Training_Data\"  #this needs to be the parent directory for the training corpus\n",
    "\n",
    "def get_training_testing(training_dir=TRAINING_DIR,split=0.5):\n",
    "\n",
    "    filenames=os.listdir(training_dir)\n",
    "    n=len(filenames)\n",
    "    print(\"There are {} files in the training directory: {}\".format(n,training_dir))\n",
    "    random.seed(53)  #if you want the same random split every time\n",
    "    random.shuffle(filenames)\n",
    "    index=int(n*split)\n",
    "    return(filenames[:index],filenames[index:])\n",
    "\n",
    "trainingfiles,heldoutfiles=get_training_testing()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainingfiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1  Building a unigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize as tokenize\n",
    "import operator\n",
    "\n",
    "class language_model():\n",
    "    \n",
    "    def __init__(self,trainingdir=TRAINING_DIR,files=[]):\n",
    "        #store the names of the files containing training data and run the training method\n",
    "        self.training_dir=trainingdir\n",
    "        self.files=files\n",
    "        \n",
    "        self.train()\n",
    "        \n",
    "    def train(self):\n",
    "        #initialise an empty dictionary which will be the unigram model {w:P(w)} when training is complete\n",
    "        self.unigram={}\n",
    "        #process all of the training data, accumulating counts of events\n",
    "        self._processfiles()\n",
    "        #convert the accumulated counts to probabilities\n",
    "        self._convert_to_probs()\n",
    "        \n",
    "    def _processline(self,line):\n",
    "        #process each line of a file\n",
    "        #each line is tokenized and has a special start and end token added\n",
    "        #counts of tokens are added to the self.unigram count model\n",
    "        tokens=[\"__START\"]+tokenize(line)+[\"__END\"]\n",
    "        for token in tokens:\n",
    "            self.unigram[token]=self.unigram.get(token,0)+1\n",
    "    \n",
    "    \n",
    "    def _processfiles(self):\n",
    "        #process each file in turn\n",
    "        for afile in self.files:\n",
    "            print(\"Processing {}\".format(afile))\n",
    "            try:\n",
    "                with open(os.path.join(self.training_dir,afile)) as instream:\n",
    "                    for line in instream:\n",
    "                        line=line.rstrip()\n",
    "                        if len(line)>0:\n",
    "                            self._processline(line)\n",
    "            except UnicodeDecodeError:\n",
    "                print(\"UnicodeDecodeError processing {}: ignoring file\".format(afile))\n",
    "      \n",
    "            \n",
    "    def _convert_to_probs(self):\n",
    "        #self.unigram initially counts counts for each token {token:freq(token)}\n",
    "        #sum all of the frequencies and divide each frequency by that sum to get probabilities\n",
    "        \n",
    "        self.unigram={k:v/sum(self.unigram.values()) for (k,v) in self.unigram.items()}\n",
    "       \n",
    "    def get_prob(self,token,method=\"unigram\"):\n",
    "        #simple look up method\n",
    "        if method==\"unigram\":\n",
    "            return self.unigram.get(token,0)\n",
    "        else:\n",
    "            print(\"Not implemented: {}\".format(method))\n",
    "            return 0\n",
    "    \n",
    "\n",
    "    \n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 19TOM10.TXT\n",
      "Processing SNOWI10.TXT\n",
      "Processing FBRLS10.TXT\n",
      "Processing WTSLW10.TXT\n",
      "UnicodeDecodeError processing WTSLW10.TXT: ignoring file\n",
      "Processing MOHIC10.TXT\n",
      "UnicodeDecodeError processing MOHIC10.TXT: ignoring file\n"
     ]
    }
   ],
   "source": [
    "MAX_FILES=5\n",
    "mylm=language_model(files=trainingfiles[:MAX_FILES])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you look up some probabilities of words in your model.  Pick some words which you would expect to have high probabilities and some words which you would expect to have low probabilities.\n",
    "\n",
    "As an extension, see how these change if you use a bigger portion of the training data to train your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "e993f6b24e6dadc761550f81b5ca4f85f65c4625eb1dc51cfa1e968282de6f17"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
